{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Comment Classifier v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2\n",
        "#!pip install torchdata==0.3.0\n",
        "#!pip install torch\n",
        "#!pip install spellchecker\n",
        "#!pip install pyspellchecker\n",
        "#!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "X2z8czhKGbz8",
        "outputId": "b4d07e6e-d0fb-4951-b418-6b19c9c1d399"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.11.2\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Collecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x399ec000 @  0x7fe4d2192615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2022.5.18.1)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "#import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "lRjYD2lzGeRf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0\n",
        "\n",
        "df_clean = pd.read_csv(\"sample_data_clean.csv\")\n",
        "df_clean.rename(columns={'toxic_any': 'label'}, inplace=True)\n",
        "df_clean = df_clean[['label', 'clean_text']]\n",
        "\n",
        "df_train, df_test = train_test_split(df_clean, test_size=0.2, random_state=23)\n",
        "df_train, df_valid = train_test_split(df_train, test_size=0.5, random_state=23)\n",
        "df_train.reset_index(inplace=True)\n",
        "df_valid.reset_index(inplace=True)\n",
        "df_test.reset_index(inplace=True)\n",
        "\n",
        "df_train.drop(columns=['index'], inplace=True)\n",
        "df_valid.drop(columns=['index'], inplace=True)\n",
        "df_test.drop(columns=['index'], inplace=True)\n",
        "\n",
        "df_train.to_csv('train.csv', index=False)\n",
        "df_valid.to_csv('valid.csv', index=False)\n",
        "df_test.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "C59RRfMf__Xz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
        "fields = [('label', label_field), ('clean_text', text_field)]\n",
        "device = None\n",
        "\n",
        "train, valid, test = TabularDataset.splits(path = './',\n",
        "                                    train='train.csv', \n",
        "                                    validation='valid.csv',\n",
        "                                    test='test.csv',\n",
        "                                    format='CSV',\n",
        "                                    fields=fields, \n",
        "                                    skip_header=True)"
      ],
      "metadata": {
        "id": "sRdRZKUvICC4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ddJwrsZAGNmP"
      },
      "outputs": [],
      "source": [
        "# Iterators\n",
        "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.clean_text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.clean_text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.clean_text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "\n",
        "# Vocabulary\n",
        "text_field.build_vocab(train, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_field.vocab.itos[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7g4JBeoDYMIL",
        "outputId": "a73c0c90-f83e-48da-df6b-058ec0c68b9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(text_field.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vDLXUPRYSdN",
        "outputId": "699891dd-f9f6-4a24-cfa0-54c410e4f673"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNK',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_default_unk_index',\n",
              " 'extend',\n",
              " 'freqs',\n",
              " 'itos',\n",
              " 'load_vectors',\n",
              " 'lookup_indices',\n",
              " 'set_vectors',\n",
              " 'stoi',\n",
              " 'unk_index',\n",
              " 'vectors']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, dimension=128):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(len(text_field.vocab), 300)\n",
        "        self.dimension = dimension\n",
        "        self.lstm = nn.LSTM(input_size=300,\n",
        "                            hidden_size=dimension,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.fc = nn.Linear(2*dimension, 1)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text_emb = self.embedding(text)\n",
        "        print('text_emb', text_emb.shape)\n",
        "\n",
        "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
        "        print('packed input', packed_input[0].shape, packed_input[1].shape)\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "        print('packed output', packed_output[0].shape, packed_output[1].shape)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        print('output', output.shape)\n",
        "\n",
        "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
        "        print('out_forward dim', out_forward.shape)\n",
        "        out_reverse = output[:, 0, self.dimension:]\n",
        "        print('out_reverse dim', out_reverse.shape)\n",
        "\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "        print('out_reduced', out_reduced.shape)\n",
        "        text_fea = self.drop(out_reduced)\n",
        "        print('text_fea', text_fea.shape)\n",
        "\n",
        "        text_fea = self.fc(text_fea)\n",
        "        print('text_fea2', text_fea.shape)\n",
        "        text_fea = torch.squeeze(text_fea, 1)\n",
        "        print('text_fea2', text_fea.shape)\n",
        "        text_out = torch.sigmoid(text_fea)\n",
        "        print('text_out', text_out.shape)\n",
        "\n",
        "        return text_out"
      ],
      "metadata": {
        "id": "jinKmJBSLgku"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "metadata": {
        "id": "7D1HZsYYNx3b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = valid_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = './',\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "      #[('toxic_any', label_field), ('comment_text', text_field)]\n",
        "        for (labels, (text, text_len)), _ in train_loader:           \n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "            output = model(text, text_len)\n",
        "            break\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  for (labels, (text, text_len)), _ in valid_loader:\n",
        "                      labels = labels.to(device)\n",
        "                      text = text.to(device)\n",
        "                      output = model(text, text_len)\n",
        "\n",
        "                      loss = criterion(output, labels)\n",
        "                      valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
        "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "\n",
        "\n",
        "model = LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model=model, optimizer=optimizer, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3h5AVFxP6lJ",
        "outputId": "94643b90-15cf-43f1-9182-0dd17c8f4351"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "text_emb torch.Size([32, 3, 300])\n",
            "packed input torch.Size([72, 300]) torch.Size([3])\n",
            "packed output torch.Size([72, 256]) torch.Size([3])\n",
            "output torch.Size([32, 3, 256])\n",
            "out_forward dim torch.Size([32, 128])\n",
            "out_reverse dim torch.Size([32, 128])\n",
            "out_reduced torch.Size([32, 256])\n",
            "text_fea torch.Size([32, 256])\n",
            "text_fea2 torch.Size([32, 1])\n",
            "text_fea2 torch.Size([32])\n",
            "text_out torch.Size([32])\n",
            "Model saved to ==> .//metrics.pt\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics('metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "WP1ZtAjGQPFh",
        "outputId": "79beed41-10a7-4905-b99e-c97d305700a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== metrics.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def evaluate(model, test_loader, version='title', threshold=0.5):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (labels, (text, text_len)), _ in test_loader:    \n",
        "            print(\"labels are\", labels)\n",
        "            print(\"text is\", text)\n",
        "            print(\"the word in vocab is\", text_field.vocab.itos[text])\n",
        "            print(\"text len is\", text_len)       \n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "            output = model(text, text_len)\n",
        "\n",
        "            output = (output > threshold).int()\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "    \n",
        "    #print('Classification Report:')\n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "    #print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['Toxic', 'Non toxic'])\n",
        "    ax.yaxis.set_ticklabels(['Toxic', 'Non toxic'])\n",
        "    \n",
        "    \n",
        "best_model = LSTM().to(device)\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
        "\n",
        "load_checkpoint('model.pt', best_model, optimizer)\n",
        "evaluate(best_model, test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "zSqYbnlse6z6",
        "outputId": "459930fb-1045-4184-e6bb-14044e99cb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== model.pt\n",
            "labels are tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
            "text is tensor([[   0,    0, 4786],\n",
            "        [ 393,  774,    0],\n",
            "        [2206, 1286,    0],\n",
            "        [ 162,  865, 3523],\n",
            "        [4230,   13,    0],\n",
            "        [   0,  981,  270],\n",
            "        [ 140,   45,  372],\n",
            "        [3150,    0,  140],\n",
            "        [ 513,    0, 2716],\n",
            "        [ 244, 1179,  164],\n",
            "        [   0,  997,   53],\n",
            "        [   0,   91,  561],\n",
            "        [ 162, 1281,  122],\n",
            "        [  66,    9,   31],\n",
            "        [   2, 2102, 5191],\n",
            "        [   0,  103,    0],\n",
            "        [ 103, 2043,   45],\n",
            "        [   3,  237,   45],\n",
            "        [ 513,   45,   57],\n",
            "        [6315, 3433,    0],\n",
            "        [   8,    0, 6245],\n",
            "        [ 656,  164,    0],\n",
            "        [1079,    3,    1],\n",
            "        [2526, 2550,    1],\n",
            "        [ 172,  490,    1],\n",
            "        [ 245,    0,    1],\n",
            "        [ 561,  164,    1],\n",
            "        [   0,  561,    1],\n",
            "        [4790,   45,    1],\n",
            "        [   0,    0,    1],\n",
            "        [ 934, 2587,    1],\n",
            "        [  58,    5,    1]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-ced17024e38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-117-ced17024e38a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader, version, threshold)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels are\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the word in vocab is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text len is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "1AVis4QQfS-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d5ad84-e0de-4797-9b18-9b97dc50dfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 313., 4163., 8341., 2732., 2359., 2154., 7312., 5682., 1442., 7152.,\n",
              "        1032., 9740., 5747., 7113., 5833., 4280., 6036., 3745., 5312., 1921.,\n",
              "         478., 9289., 7476., 3007., 2787., 7701., 8465., 9403., 9659., 8731.,\n",
              "        1394., 1482.])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}